[{"categories":["Software Engineering"],"contents":"A while ago I was working as a contractor for an e-commerce company. There were only a couple of developers, and a lot of work to be done. In normal conditions there would have been enough people to do the job. Unfortunately, there was one issue: making changes was incredibly painful. The existing codebase was quite large and had no tests. Not even a single one. The team was basically living through the day, spending the majority of time fixing bugs rather than adding new features. Bugs were not fixed through extensive refactoring, solving the root cause of the problem, but by adding patches that introduced more issues than they were solving. APIs were not clearly defined, which led to changes in one component to break a totally unrelated part of the project. Deployments were a completely manual and error-prone process, and the only way to deploy the app on a new machine was to install everything yourself, from scratch.\nI was still a university student at that point. In fact, this was my very first job as a developer. Making seemingly trivial changes was so hard that I began to seriously question my skills as a software engineer. It was my first time looking at a \u0026ldquo;real-world\u0026rdquo; codebase, so I just assumed I was the problem. Years after that, I now know I wasn't the problem. Working on such a codebase using almost all the bad practices of modern software development would be frustrating for anyone. I'm sure many of you have had such an experience.\nWorking at a top-tier tech company such as Lyft, I got the chance to learn and employ best practices on really high-quality codebases. The development experience has been completely different for me ever since. There is a lot to be said about the things I have learned at Lyft, and I could only scratch the surface here. This article focuses on one of those things: Continuous Integration (CI). I'll tell you the main advantages of having a CI system, and show you a basic Travis CI setup with one of my public github repos.\nContinuous Integration For those of you who never heard about it, Continuous Integration is a software development practice where each person's changes are integrated very quickly, often within hours, into a shared development state, often the master branch of a VCS repository. Crucial to the practice is that each integration has automated tests that check for correctness, and scripts that builds the artifacts that are needed for deployment. There are many advantages of adopting CI. On the top of my head, I can think of at least these ones:\n Much faster development iterations Tighter feedback loop on all changes, making it easier to debug issues and pinpoint what caused what Less conflicts between people's changes  Continuous \u0026ldquo;*\u0026rdquo; You might have heard different terms matching the Continuous * regular expression. The most common ones are Integration, Delivery, and Deployment. The differences are:\n Continuous Integration runs an automated suite that builds the software and tests your change. If everything goes well, you are allowed to integrate the change to the master branch. No production deploys are involved in CI Continuous Delivery is Continuous Integration plus the things required for a production deployment. The actual deployment is not triggered, but only prepared. Someone from the business usually decides when and how often to deploy, which is usually triggered manually Continuous Deployment is Continuous Delivery plus the actual production deployment is also automated  There's a lot to be said about the differences between them, and how they fit to different use-cases, but in this post we are only talking about the first one.\nWhy you can't simply \u0026ldquo;add CI to my project\u0026rdquo; The truth about CI is, you can't simply \u0026ldquo;add it\u0026rdquo;. There are certain requirements your project needs to meet. In particular, you can't be talking about CI if you have not automated building and testing your software. If a build requires manual steps, or if tests are not comprehensive enough to give you confidence in your changes, then you can't effectively implement CI. Two things at least need to happen:\n An automated build process, ideally in a single command A set of tests that give you enough confidence in the quality of your change  The build script should include everything your system needs to run. If the server needs a database, it should be created from scratch, including the schema. It should not rely on some existing infrastructure. In addition to that, and I cannot stress enough how important this is, builds should be fast. If it takes hours to build then you can't reap all the benefits CI has to offer. This is usually achieved by only building the components that actually changed, or by not building things that are completely unrelated to your change.\nTravis CI There exist many CI solutions out there, each one with its pros and cons. The reasons I use Travis CI for my personal projects are:\n It's a hosted SaaS, meaning I don't have the hassle of hosting it myself It's free for open source projects It's widely used, meaning that the problems I will inevitably encounter are already solved by someone else  I'm not going to go into the details of how to setup Travis CI, because their getting started guide is very well written and easy to follow.\nOnce you're all set up, the next step is to create a .travis.yml file in the root of your repository. This is the main configuration file: it tells Travis what to do, how to do it and where to do it. I have configured my python package template repo to use Travis CI, and here is how the configuration looks like:\nlanguage: python python: - \u0026quot;3.7\u0026quot; install: - ./setup.sh - pipenv install --deploy --dev script: - ./test.sh In a nutshell: language says that the project uses Python, which will make Travis run inside a virtual envionment created on purpose for the build. (python) specifies which Python version(s) we want to test with. You can also put multiple versions in there, and the tests will be run against all of them. (install) tells Travis how to build the software. The default for Python if you omit this line is pip install -r requirements.txt, but since I have a custom setup script and I use Pipfile instead of requirements.txt, I had to override that config. (script) tells Travis what to run - in this case a custom script I made which runs linting, type-checking and unit-testing. If all of this runs correctly, it means that:\n The software still builds, as in I haven't introduced conflicts between dependencies or anything like that My change doesn't break any existing code (this of course depends on the quality of the tests I write)  One small tip: when I first added the .travis.yml file, builds were not being triggered for my pull requests and I couldn't understand why. It turned out there was a syntax error in the file, which I discovered by looking at the Requests section of the Travis CI dashboard. You can access it through the dropdown menu on the right:\nFinal thoughts This article only scratches the surface of the things you can do with CI. The benefits of the approach can hardly be overstated, and it will make your team save an incredible amount of time, and at the same time increasing the quality of the software you deliver. I encourage you to read more about the topic. One pointer I can give you is this great paper by Martin Fowler, which is the best explaination of the technique I have ever come across. And if you are not using CI, I strongly recommend that you give it a try.\nDisclaimer: this article is not about any specific technology or approach that is actually in use at Lyft. It's just a small summary of what I have learned about CI while working there.\n","permalink":"https://gabrieleangeletti.github.io/blog/things-i-have-learned-at-lyft-continuous-integration/","tags":["Lyft","Continuous Integration","Travis CI"],"title":"Things I have learned at Lyft: Continuous Integration"},{"categories":["Software Engineering"],"contents":"I am a lazy person. Every time I find myself doing the same thing more than twice, I automate it. It requires effort at first, but it is worth it in the long run. Starting a new Python project is one of those things, and today I wanna share with you my blueprint for it. You can find the full template on github. This setup is a good starting point for a small to medium-sized codebase and it does the common stuff:\n Set up the development environment Manage dependencies Format your code Run linting, static type-checking and unit-testing  In the next sections I will describe how these things are set up in the template. Note that a few things are missing, which I'm planning to add next:\n A deployment script A continuous integration pipeline  Managing python versions - Pyenv Managing multiple versions of python, or of any language for that matter, is a painful experience, for many reasons: the system version that you can't touch, the 2 vs 3 nightmares, two different projects that require different interpreters and so on. Pyenv solves this problem: it is a version management tool that makes your life easier in a lot of ways. If you come from the Javascript / Node world, the tool is similar to the popular n.\nWith pyenv you can easily:\n Install a new version: pyenv install X.Y.Z Set a version as global: pyenv global X.Y.Z Set a version for the current shell by overriding the PYENV_VERSION environment variable Set an application-specific version by creating a .python-version file  Managing dependencies - Pipfile If dealing with versions is painful, dealing with dependencies is even worse. Any non-trivial application depends on external packages, which in turn depend on other packages, and ensuring everyone gets the same versions can be rather challenging. In the python world, dependencies have been traditionally managed through the requirements.txt file. It contains the packages your app needs, optionally with the required versions. The problem is that this file doesn't handle recursive dependencies, that is the dependencies of your app's dependencies. Pipfile is a new specification that aims to solve this. It has many advantages over requirements. The biggest one by far is deterministic builds. Pipfile and its partner Pipfile.lock contain all the information needed to install the same exact environment anywhere.\nLet's make an example. Consider the following scenario: our application Ninja ducks depends on version 1.2.3 of package ninja, which in turn depends on another package called requests.\nExample - requirements.txt A requirements.txt file would look like this:\nninja==1.2.3 When running pip install -r requirements.txt, we install version 1.2.3 of ninja, because that's what the requirements say, and version 2.7.9 of requests, because that was the latest public version at the time. A couple of weeks later we deploy the application, but in the meantime requests was upgraded to 3.0.0. If ninja was using a feature from requests that has been changed or removed, our application will crash. We could fix this problem by adding requests to the requirements file, but you can see for yourself that this solution doesn't really scale.\nExample - Pipfile A Pipfile instead would look something like this:\n[[source]] url = \u0026quot;https://pypi.org/simple\u0026quot; verify_ssl = true name = \u0026quot;pypi\u0026quot; [packages] ninja = {version = \u0026quot;==1.2.3\u0026quot;} From this we can run pipenv lock to generate a Pipfile.lock:\n{ \u0026quot;_meta\u0026quot;: { \u0026quot;hash\u0026quot;: { \u0026quot;sha256\u0026quot;: \u0026quot;2aa45098c4b406ce8fccc5d6abfd7fcfd0e39cc6b6ef8529d1e14882d86f007c\u0026quot; }, \u0026quot;pipfile-spec\u0026quot;: 6, \u0026quot;sources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;pypi\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://pypi.org/simple\u0026quot;, \u0026quot;verify_ssl\u0026quot;: true } ] }, \u0026quot;default\u0026quot;: { \u0026quot;ninja\u0026quot;: { \u0026quot;hashes\u0026quot;: [ \u0026quot;sha256:37228cda29411948b422fae072f57e31d3396d2ee1c9783775980ee9c9990af6\u0026quot;, \u0026quot;sha256:58587dd4dc3daefad0487f6d9ae32b4542b185e1c36db6993290e7c41ca2b47c\u0026quot; ], \u0026quot;version\u0026quot;: \u0026quot;==1.2.3\u0026quot; }, \u0026quot;requests\u0026quot;: { \u0026quot;hashes\u0026quot;: [ \u0026quot;sha256:9e5896d1372858f8dd3344faf4e5014d21849c756c8d5701f78f8a103b372d92\u0026quot;, \u0026quot;sha256:d8b24664561d0d34ddfaec54636d502d7cea6e29c3eaf68f3df6180863e2166e\u0026quot; ], \u0026quot;version\u0026quot;: \u0026quot;==2.7.9\u0026quot; } } } As you can see, requests is there, even if we didn't mention it anywhere in our Pipfile. That's because Pipfile handles recursive dependencies through the Pipfile.lock file. During deploy, when we run pipenv install --deploy to install dependencies, the correct version of requests will be installed, regardless of the latest version available in the public registry.\n Note 1: in the above I used a couple of pipenv commands, which is the reference implementation for the Pipfile specification Note 2: you need to add both Pipfile and Pipfile.lock to your repository, otherwise you will not be able to restore the same environment Note 3: if you are currently using requirements.txt and want to migrate to Pipfile, here's an handy guide on how to do it  In the template, both pyenv and pipenv can be installed through the ./setup.sh script provided. It only supports Linux and MacOS (some packages need to be installed manually on Linux).\nManaging code - my favourite tools Here's a list, in no specific order, of the code quality tools I always use in my Python projects.\nFormatting - Black According to this book I have recently read, willpower is a limited resource. It is like a muscle, you can't just stay focused for an entire day and expect the same level of productivity all along . That's why when programming, I want to use my time thinking on the important stuff, not on indentation, brackets, and so on. Everything that can be automated must be automated. I can see at least two major benefits of code formatting:\n You cede control over formatting rules to the tool, which means you stop thinking about it Since everyone is onboard, you stop discussing with your team whether the perfect line length should be 42, 79 or 110 (or at least you have just one big discussion at the beginning)  Black refers to itself as \u0026ldquo;the uncompromising Python code formatter\u0026rdquo;, and it is my favourite formatting tool. It is super simple to use, just run:\nblack {source directory} Black has a lot of configurable options. The only one I use is a line length of 110. If you check the full project template, I have included a handy ./format_code.sh script that will format your code in a single command.\nLinting - Flake8 Linting is a rather basic code quality check that helps prevent simple bugs in your code. Things like typos, bad formatting, unused variables and so on. To me linting is super useful because:\n You don't have to check for minor details, hence you save time Other developers don't have to check for minor details, hence they save time  I use flake8 for linting. One feature I particularly like is the ability to ignore specific warnings and errors. For instance, I use a line length of 110 which is against the PEP8 style guide (79 is recommended). By turning off the corresponding error E501 I can safely use flake8 with any desired line length.\nIn the project template, you can run flake8 against your package with ./test.sh lint.\nType checking - Mypy This is my favourite by far. This tool brings static type checking to the python world. I've never written a single line of untyped python since I've discovered mypy. I'm not gonna go into the details of static type checking, I'll just show you a simple example stolen from mypy's website:\nStandard python:\ndef fibonacci(n): a, b = 0, 1 while a \u0026lt; n: yield a a, b = b, a+b Typed python:\ndef fibonacci(n: int) -\u0026gt; Iterator[int]: a, b = 0, 1 while a \u0026lt; n: yield a a, b = b, a+b This is super useful because:\n I'm much more likely to understand what a function does by looking at its signature I can spot lots of errors before even running the code I can check whether I'm correctly using a third-party library  In the project template, you can run mypy against your package with ./test.sh type_check.\nTesting - Pytest Pytest is the best testing framework for python. It gives you detailed info on why your tests are failing, can auto-discover your tests based on their name, has an amazing support for fixtures, and a lot of useful plugins. Writing tests is super easy with pytest. Consider the following module my_module.py:\ndef my_func(x: int) -\u0026gt; int: return x ** 2 To test this function, we create a module named my_module_test.py:\nfrom . import my_module def test_my_func(): expected = 9 actual = my_module.my_func(3) assert actual == expected The main features I use from pytest, in random order, are:\n pytest-cov: plugin that generates test coverage reports for your code, in a variety of formats pytest-mock: plugin that adds a fixture for monkey-patching. Usage:  def test_my_func(mocker): mocker.patch(...)  pytest-xdist: run unit-tests in parallel. Especially useful for large codebases pytest's marking feature. You can label tests by using a decorator:  import pytest @pytest.mark.integration def test_my_integration_test(): ... Then you can run only the tests labeled as integration.\nIn the project template, you can run pytest against your package with ./test.sh unit_tests.\nOthers Other dev. tools that I use in my projects are:\n isort: automatically sorts imports and separates them into sections: internal, first-party, third-party etc. Again, I'm all about automation, and this tool removes another thing from my mind vulture tool to check for dead code, like unused functions, unused constants etc. It's nice to keep your house clean, especially if you know about the broken windows theory  Please let me know your opinion in the comments below. The full code can be found here (instructions are in the readme).\n","permalink":"https://gabrieleangeletti.github.io/blog/python-project-template/","tags":["Python"],"title":"A template for Python projects"},{"categories":null,"contents":"Hi, I'm Gabriele! A software engineer and rock climbing addict on a journey to location independence.\nI have been living in London for the past couple of years, working on self-driving cars at Lyft. I was enjoying my life there, but I've always had this inner need to experience more of the beauty the world has to offer. That's why I left my job and embarked on this journey to become location independent. Or digital nomad, as they call it these days.\nI was never a big traveller. I never left Europe until I was 26, when I went to India. Then Indonesia this year, and after that it was a matter of time before I would leave everything and give it a shot.\nWhen not programming, I spend my time either climbing or searching for the best coffee out there. Also, you can often find me trying to cook some exotic asian dish.\nI use this blog to share thoughts and blabbering about software engineering topics. I hope you will learn something from it. If you do, I would love to hear about it.\nAlso, I'm available for hiring!\n","permalink":"https://gabrieleangeletti.github.io/about/about/","tags":null,"title":"About Me"}]