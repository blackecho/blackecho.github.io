<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gabriele Angeletti</title>
    <description>My blog about Artificial Intelligence, Machine Learning and related topics.
</description>
    <link>http://www.gabrieleangeletti.com/</link>
    <atom:link href="http://www.gabrieleangeletti.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 20 Feb 2016 18:34:30 +0100</pubDate>
    <lastBuildDate>Sat, 20 Feb 2016 18:34:30 +0100</lastBuildDate>
    <generator>Jekyll v3.0.3</generator>
    
      <item>
        <title>Sparse Distributed Representations in Machine Learning</title>
        <description>&lt;p&gt;Today we will take a look at &lt;a href=&quot;https://github.com/numenta/nupic/wiki/Sparse-Distributed-Representations&quot;&gt;Sparse Distributed Representations&lt;/a&gt;: what they are, why they differ from the standard types of representations used by computers, and why many people think they are fundamental in building true machine intelligence.&lt;/p&gt;

&lt;p&gt;Computers manipulate information through so called &lt;strong&gt;dense&lt;/strong&gt; representations: basically, they use a small number of bits and every combination of them represent something. For example, think about &lt;a href=&quot;https://en.wikipedia.org/wiki/ASCII&quot;&gt;ASCII&lt;/a&gt; code. It uses a fixed number of bits, and any combination of those bits represents something, for instance ‘110 0111’ is ‘g’.&lt;/p&gt;

&lt;p&gt;There are two things that make us thing this kind of representations are not suitable for machine learning applications.
First, the individual bits has no meaning at all. They become meaningful only when they are put together. For example, pick the code corresponding to ‘g’ in ASCII. Someone has chosen that combination of bits to represent a ‘g’, but the fact that the third bit is 0 says anything about the concept of ‘g’.
Second, this kind of representations is not fault tolerant: if you change only one bit, you obtain a completely different meaning.&lt;/p&gt;

&lt;p&gt;Neuroscientists have discovered that this is not how the human brain works. Instead, the brain uses a Sparse Distributed Representation of information.&lt;/p&gt;

&lt;p&gt;You can think of a binary string as a set of neurons, where each neuron can be active (1) or not active (0). These representations are &lt;strong&gt;sparse&lt;/strong&gt; in the sense that at any given time, only a tiny fraction of the neurons are active, so you always have a very small percentage of 1s. They are also &lt;strong&gt;distributed&lt;/strong&gt; in the sense that they are a many-to-many mapping between concepts and neurons: each concept is represented by many neurons, and each neuron participates in the representation of many concepts. Here individual neurons have semantic meaning.&lt;/p&gt;

&lt;p&gt;For example, if you want to represent the concept of a car with a distributed representation, you can use a binary vector of length 1000, where each element represent the presence or absence of the feature it represent. In this sense neurons have semantic meaning: if the neuron in position 49 is active, you know that that car is red, regardless of all the other neurons.&lt;/p&gt;

&lt;p&gt;In ASCII code, the fact that the second bit is 1 is almost irrelevant. Instead, in a distributed representation, this is meaningful, in fact you can say that two sequences are semantically similar if they share some indexes of active bits, the more they share the more they are similar. So you can even sample a subset of the bits and compare only those samples to infer similarity (this is one of the advantages of a distributed representation). They have another advantage: because the representation is sparse, you can store only the indices of the active bits, and because there a few active bits at a given time, you can save a lot of space!&lt;/p&gt;

&lt;p&gt;Sparse distributed representations are used by many algorithms: they are at the core of a new
brach of Artificial Intelligence algorithms, &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;Deep Learning&lt;/a&gt;, that is having huge success in the field, beating previous state-of-the-art algorithms in almost all branches, from image to audio to text.
There are many kind of Deep Learning Neural Networks, but all of them are based on the concept of
unsupervised pretraining: you train one layer at a time in an unsupervised fashion, and the output of one layer is used as training set for the next layer in the hierarchy. All these layers learn to detect &lt;strong&gt;features&lt;/strong&gt; of the training data, the higher the layer in the hierarchy, the more abstract and meaningful the features are expected to be. For instance, if we are recognizing faces in images, in the first layer one would expect to see edges, strokes, blobs, while in the second layer one would expect to see something like an eye, a mouth, and so no.
All these layers are indeed learning Sparse Distributed Representations of the training set, and this is way this is useful to know and study this topic in order to grasp the concepts behind Deep Learning.&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/

var disqus_config = function () {
this.page.url = &quot;www.gabrieleangeletti.com/2015/06/sparse-distributed-representations&quot;; // Replace PAGE_URL with your page&#39;s canonical URL variable
this.page.identifier = &quot;Sparse Distributed Representations in Machine Learning&quot;; // Replace PAGE_IDENTIFIER with your page&#39;s unique identifier variable
};

(function() { // DON&#39;T EDIT BELOW THIS LINE
var d = document, s = d.createElement(&#39;script&#39;);

s.src = &#39;//gabrieleangeletti.disqus.com/embed.js&#39;;

s.setAttribute(&#39;data-timestamp&#39;, +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot; rel=&quot;nofollow&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;
</description>
        <pubDate>Mon, 15 Jun 2015 16:30:20 +0200</pubDate>
        <link>http://www.gabrieleangeletti.com/2015/06/sparse-distributed-representations</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/2015/06/sparse-distributed-representations</guid>
        
        
        <category>machine-learning</category>
        
      </item>
    
      <item>
        <title>The Forward-Backward Algorithm for Hidden Markov Models</title>
        <description>&lt;p&gt;Today we will see how we can do inference in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_Model&quot;&gt;Hidden Markov Model (HMM)&lt;/a&gt;, using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm&quot;&gt;Forward-Backward (FB) algorithm&lt;/a&gt;. First of all I will briefly recap what a HMM is, then I’ll show you the algorithm in detail.&lt;/p&gt;

&lt;p&gt;A HMM is a statistical model that can be used to do a lot of interesting things, it has many applications in speech-recognition, handwriting recognition, and many others.
The basic idea is that if we have some observations about something, they can be explained in terms of some states (which are hidden) to which these observations depend. If we take for example handwriting recognition, the hidden states would be the letters of the alphabet, and the observed variables would be the letters that one actually writes.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;responsive-img&quot; src=&quot;../../img/hmm.png&quot; alt=&quot;hidden markov model graphic representation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The arrows in the image indicate a conditional probability, so there are conditional probabilities between states and between
states and observations.
These two quantities themselves, together with the initial distributions for the states, entirely define an HMM.
So you have two matrices, the transition probability matrix $T$ (or transition model) and the observation probability matrix $O$
(or observation model).&lt;/p&gt;

&lt;p&gt;$T$ contains the probabilities of changing state: if the model has N states, then the matrix will have dimensions NxN, in which the $T_{(i,j)}$ cell contains the probability that the next state will be j, given that the current state is i.&lt;/p&gt;

&lt;p&gt;$O$ contains the probabilities of observing a particular value, given the current state: for example the $O_{(i,j)}$ cell
contains the probability of observing value i given that the current state is j.&lt;/p&gt;

&lt;p&gt;When you have defined a HMM, you can compute the posterior probability distribution of the states given the observations, thus estimating the
correlations between the observed variables and the hidden states. In the handwriting example this means that we would be able to recognize the character
that one has written given the actual messy character that she/he wrote.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The FB algorithm allows you to compute these posterior probabilities $P(Z_k&lt;/td&gt;
      &lt;td&gt;X_{1:t})$ for internal hidden states.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The basic idea is that this probability is proportional to the joint distribution $ P(Z_k,X_{1:t}) $ and, using &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayes&#39;_theorem&quot;&gt;Bayes theorem&lt;/a&gt; and the independence of $ X_{1:k} $ and $ X_{k+1:t} $ given $ Z_k $, we obtain:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Z_k | X_{1:t}) \propto P(Z_k , X_{1:t}) = P(Z_k | X_{1:k}) P(X_{k+1:t} | Z_k)&lt;/script&gt;

&lt;p&gt;This algorithm rely on the idea of &lt;a href=&quot;http://en.wikipedia.org/wiki/Dynamic_programming&quot;&gt;Dynamic Programming&lt;/a&gt;, that is a fancy name for saying that we reuse things we have already computed to make things faster.&lt;/p&gt;

&lt;p&gt;The forward part computes the posterior probability of state $Z_k$ given the first k observations $ X_{1:k}$, and does
this $\forall k = 1 … n$.&lt;/p&gt;

&lt;p&gt;The backward part computes the probability of the remaining observations $X_{k+1:n}$ given state $Z_k$, again for each k.&lt;/p&gt;

&lt;p&gt;The last part of the algorithm is to merge the forward and the backward part to give the full posterior probabilities.&lt;/p&gt;

&lt;p&gt;This algorithm gives you probabilities for all hidden states, but it doesn’t give probabilities for sequences of states (see the &lt;a href=&quot;http://en.wikipedia.org/wiki/Viterbi_algorithm&quot;&gt;Viterbi Algorithm&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;forward-part&quot;&gt;Forward Part&lt;/h2&gt;

&lt;p&gt;We want to use dynamic programming, so we want to express $P(Z_k | X_{1:k})$ as a function of k-1, so that we can setup a recursion. We achieve that by using &lt;a href=&quot;http://en.wikipedia.org/wiki/Marginal_distribution&quot;&gt;marginalization&lt;/a&gt;.
So using marginalization we can bring $ Z_{k-1} $ to the equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Z_k,X_{1:k}) = \sum_{Z_{k-1}} P(Z_k, Z_{k-1}, X_{1:k-1}, X_k)&lt;/script&gt;

&lt;p&gt;Where the sum is over all the values which $ Z_{k-1} $ can assume. Now we can use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Chain_rule_(probability)&quot;&gt;chain rule for probability&lt;/a&gt;&amp;lt;/a&amp;gt; and see what happens:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{Z_{k-1}} P(Z_k, Z_{k-1}, X_{1:k-1}, X_k) = \sum_{Z_{k-1}} P(X_k | Z_k, Z_{k-1}, X_{1:k-1}) P(Z_k|Z_{k-1}, X_{1:k-1}) P(Z_{k-1} | X_{1:k-1}) P(X_{1:k-1})&lt;/script&gt;

&lt;p&gt;the last two terms can be unified in: $P(Z_{k-1}, X_{1:k-1})$. So now we use &lt;a href=&quot;http://www.andrew.cmu.edu/user/scheines/tutor/d-sep.html&quot;&gt;D-separation&lt;/a&gt; to simplify the above equation. In the first term we can remove $ Z_{k-1}$ and $ X_{1:k-1} $, because $ X_k $ is independent of them, given $ Z_k $. We can see it graphically using D-separation: the idea is that any path in the graph from $ X_k $ to $ Z_{k-1}$ or $ X_{1:k-1} $, must pass through $ Z_k $, so we say that they are independent given $ Z_k $ and we remove them.
The same goes for the second term, in which we remove $ X_{1:k-1} $. Let’s now write the simplified equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Z_k,X_{1:k}) = \sum_{Z_{k-1}} P(X_k | Z_k) P(Z_k|Z_{k-1}) P(Z_{k-1}, X_{1:k-1})&lt;/script&gt;

&lt;p&gt;Let’s examine this equation. The first term is the emission probability, which we have assumed we know. The second term is the transition probability, which also we have assumed we know. The third term is the recursion term over k-1, so we compute this value $\forall k = 2 … n$ and we are done.
For k = 1, the value is simple to compute:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Z_1,X_1) = P(X_1 | Z_1) P(Z_1)&lt;/script&gt;

&lt;h2 id=&quot;backward-part&quot;&gt;Backward Part&lt;/h2&gt;

&lt;p&gt;For the backward part, we compute the values in a similar way, by using marginalization and the chain rule.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X_{k+1, t}, Z_k) = \sum_{Z_{k+1}} P(X_{k+1:t}, Z_{k+1} | Z_k) = \sum_{Z_{k+1}} P(X_{k+1}, X_{k+2:t}, Z_{k+1} | Z_k)&lt;/script&gt;

&lt;p&gt;Now we again use the chain rule and see what happens:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{Z_{k+1}} = P(X_{k+2:t} | Z_{k+1}, X_{k+1}, Z_k) P(X_{k+1} | Z_{k+1}, Z_k) P(Z_{k+1}, Z_k)&lt;/script&gt;

&lt;p&gt;Like in the forward part, we can use D-separation to simplify the equation. In the first term, we can remove $ X_{k+1}$ and $Z_k$, and in the second
term we can remove $Z_k$. So now the equation is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{Z_{k+1}} = P(X_{k+2:t} | Z_{k+1}) P(X_{k+1} | Z_{k+1}) P(Z_{k+1}, Z_k)&lt;/script&gt;

&lt;p&gt;And again: we have an emission probability (second term), a transition probability (third term), and a recursion term over k+1 (first term).
We again compute this value $\forall k = 1 … n-1$ and we are done. For k=n, the value is 1 (you can compute it yourself is pretty easy).&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/

var disqus_config = function () {
this.page.url = &quot;www.gabrieleangeletti.com/2015/05/forward-backward-algorithm-hidden-markov-models&quot;; // Replace PAGE_URL with your page&#39;s canonical URL variable
this.page.identifier = &quot;The Forward-Backward Algorithm for Hidden Markov Models&quot;; // Replace PAGE_IDENTIFIER with your page&#39;s unique identifier variable
};

(function() { // DON&#39;T EDIT BELOW THIS LINE
var d = document, s = d.createElement(&#39;script&#39;);

s.src = &#39;//gabrieleangeletti.disqus.com/embed.js&#39;;

s.setAttribute(&#39;data-timestamp&#39;, +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot; rel=&quot;nofollow&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;
</description>
        <pubDate>Sat, 30 May 2015 17:30:20 +0200</pubDate>
        <link>http://www.gabrieleangeletti.com/2015/05/forward-backward-algorithm-hidden-markov-models</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/2015/05/forward-backward-algorithm-hidden-markov-models</guid>
        
        
        <category>machine-learning</category>
        
      </item>
    
      <item>
        <title>A Breadth-First Search (BFS) Prolog meta-interpreter</title>
        <description>&lt;p&gt;Today we will build a meta-interpreter for Prolog that uses a Breadth-First search strategy. The Prolog built-in interpreter uses DFS by default to build the tree of computations. This is due to performance reasons, because other searching policies wouldn’t be feasible, except for fairly easy problems. But a cool fact about Prolog is that code and data are basically the same thing, and because of that its easy to implement a Prolog program that runs other Prolog programs (i.e. a meta-interpreter).
We can build meta-interpreters to change the behavior of our interpreter: we can for example change the search policy, do the occur check, return the depth of the computations tree, and many other things.&lt;/p&gt;

&lt;p&gt;In this post we will change Prolog’s search strategy from DFS to BFS, as an example of how we can easily change its behavior.
The main difference between DFS and BFS is that DFS uses a Stack to mantain the list of nodes to visit, and it means that DFS always tries to expand the last node it has visited. This policy can turn into trouble if there is a
left recursion, which is when a variable that appears in the head of a rule also appears in the first goal of the tail. Since Prolog doesn’t do the occur check, which is needed exactly to avoid such situations, it loops forever, until it goes out of local stack.&lt;/p&gt;

&lt;p&gt;With Breadth-First Search such situations can be avoided, because BFS will expand all the goals in a clause, instead of keep trying to expand the first node it encounter.
This is due do the fact that BFS uses a Queue to mantain the list of nodes to visit, so it when it expands a node, it won’t visit what it has just expanded, but it will continue to visit the other nodes it hasn’t expanded yet.&lt;/p&gt;

&lt;p&gt;So at each depth level, BFS will first visit all the nodes at that level, and then move to the next level. While this is bad for performance, because we don’t want any useless path to be visited, it can always find a solution if it exists, thus BFS is sound and complete. Let’s now implement the meta-interpreter.&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p&gt;We will use Prolog’s built-in predicate clause/2.
clause(Goal, Body) unifies Body with the bodies that matches with Goal, and return them in the form (FirstGoal, OtherGoals), where OtherGoals can be another list of the form (FirstGoal, OtherGoals). So clause/2 will find for us all the matches with Goal. Now we need a predicate that converts goal lists of this form into a standard prolog list with square brackets. To do this we will implement the predicate conj_goals_2_list.
Now we are ready to implement the meta-interpreter. Basically BFS is the same as DFS except that it uses a Queue instead of a Stack. The Queue works with a FIFO (first-in first-out) policy, so we always insert element at the last position of the list, and we always retrieve elements at the first position.
In Prolog we can do this pretty easily by using the way it handles lists and append/3.
Using the predicate append(OldList, Element, NewList), NewList will be the result of adding Element at the end of OldList.
So the code looks like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;solveBFS([Goal|Rest]) :- % list of goals to prove
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;we use clause/2 on the first element of the goals list, that is the first-out part of FIFO&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;clause(Goal, Body)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;we convert the list format returned by clause into a standart Prolog list&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conj_goals_2_list(Body, List)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;we append List to the end of Rest, that is the part first-in part of FIFO&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;append(Rest, List, New)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;we call recursively the solver passing it the new list. The program will keep trying to prove the goals, until all the goals are proved, or they can’t be proved&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;solveBFS(New)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The full code is available at &lt;a href=&quot;http://github.com/blackecho/prolog-programs&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/

var disqus_config = function () {
this.page.url = &quot;www.gabrieleangeletti.com/2015/05/prolog-bfs-meta-interpreter&quot;; // Replace PAGE_URL with your page&#39;s canonical URL variable
this.page.identifier = &quot;A Breadth-First Search (BFS) Prolog meta-interpreter&quot;; // Replace PAGE_IDENTIFIER with your page&#39;s unique identifier variable
};

(function() { // DON&#39;T EDIT BELOW THIS LINE
var d = document, s = d.createElement(&#39;script&#39;);

s.src = &#39;//gabrieleangeletti.disqus.com/embed.js&#39;;

s.setAttribute(&#39;data-timestamp&#39;, +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot; rel=&quot;nofollow&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;
</description>
        <pubDate>Thu, 21 May 2015 16:30:20 +0200</pubDate>
        <link>http://www.gabrieleangeletti.com/2015/05/prolog-bfs-meta-interpreter</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/2015/05/prolog-bfs-meta-interpreter</guid>
        
        
        <category>prolog</category>
        
      </item>
    
      <item>
        <title>Solving Sudoku as a Constraint Satisfaction Problem (CSP) using Prolog</title>
        <description>&lt;p&gt;Today we will model the game of &lt;a href=&quot;http://en.wikipedia.org/wiki/Sudoku&quot;&gt;Sudoku&lt;/a&gt; as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Constraint_satisfaction_problem&quot;&gt;Constraint Satisfaction Problem (CSP)&lt;/a&gt; and then write a Prolog program that solves it.
CSP is particularly useful when we want to model a combinatorial problem subject to &lt;strong&gt;constraints&lt;/strong&gt;. In standard Sudoku we have a 9x9 grid, where each cell can be filled with an integer between 1 and 9, so the possible configurations are all the ordered combinations of 81 numbers between 1 and 9.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../img/sudoku.png&quot; width=&quot;50%&quot; style=&quot;display: block;margin: 0 auto;clear right;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the first cell, we have 9 different values that we can put in.
If we consider both the first and the second cells, we have $ 9^{2} $ different values that we can put in, because we have to take into account all the possible combination of
the form $ (1,1), (1,2) … (1,9), (2,1) … (9,9) $. So if we consider all the cells in the end we have $ 9^{81} $ possible combination of values, a very huge number!!&lt;/p&gt;

&lt;p&gt;But, two things help us here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sudoku is not about trying some random combination, it has some rules, and those rules greatly reduce the above number, because they tell us which combinations are right, and which are wrong.&lt;/li&gt;
  &lt;li&gt;Sudoku has some cells that are already filled, and these &lt;strong&gt;initial conditions&lt;/strong&gt; also help in reducing the huge number of tries.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So let’s see how we can model this game in csp.&lt;/p&gt;

&lt;p&gt;CSP has variables (what we are interested in), domains (which values the variables can assume) and constraints (rules about what values are right and what are wrong).
The variables in this case are the 81 cells of the grid, each of which can assume a value between 1 and 9, so the domain here is the integer interval [1, 9].
The constraints simply are the rules of the game, that are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All the values in each row must be different each other&lt;/li&gt;
  &lt;li&gt;All the values in each column must be different each other&lt;/li&gt;
  &lt;li&gt;All the values in each 3x3 block must be different each other&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Done! We don’t need some fancy algorithm or complex trick to get the job done: we are simply describing the problem here, and we let Prolog’s backtracking solve it for us. This is the true power of Prolog and logic
Programming: you don’t actually have to solve the problem, you only have to model it!&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p&gt;Prolog has a csp library, named &lt;strong&gt;clpfd&lt;/strong&gt;. It has all the things that we need: we can define variables, domains for the variables, and costraints. More over, the constraints that the values must be different, is a type of constraint that is already implemented in clpfd. All we have to do is to use all_different(List).&lt;/p&gt;

&lt;p&gt;The full code is available at &lt;a href=&quot;http://github.com/blackecho/prolog-programs&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/

var disqus_config = function () {
this.page.url = &quot;www.gabrieleangeletti.com/2015/05/sudoku-csp-prolog&quot;; // Replace PAGE_URL with your page&#39;s canonical URL variable
this.page.identifier = &quot;Solving Sudoku as a Constraint Satisfaction Problem (CSP) using Prolog&quot;; // Replace PAGE_IDENTIFIER with your page&#39;s unique identifier variable
};

(function() { // DON&#39;T EDIT BELOW THIS LINE
var d = document, s = d.createElement(&#39;script&#39;);

s.src = &#39;//gabrieleangeletti.disqus.com/embed.js&#39;;

s.setAttribute(&#39;data-timestamp&#39;, +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot; rel=&quot;nofollow&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;
</description>
        <pubDate>Thu, 14 May 2015 21:30:20 +0200</pubDate>
        <link>http://www.gabrieleangeletti.com/2015/05/sudoku-csp-prolog</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/2015/05/sudoku-csp-prolog</guid>
        
        
        <category>prolog</category>
        
      </item>
    
  </channel>
</rss>
