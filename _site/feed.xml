<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gabriele Angeletti</title>
    <description>My blog about Artificial Intelligence, Machine Learning and related topics.
</description>
    <link>http://www.gabrieleangeletti.com/</link>
    <atom:link href="http://www.gabrieleangeletti.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 25 Mar 2016 19:38:18 +0100</pubDate>
    <lastBuildDate>Fri, 25 Mar 2016 19:38:18 +0100</lastBuildDate>
    <generator>Jekyll v3.1.1</generator>
    
      <item>
        <title>Understanding Python&#39;s underlying data structures</title>
        <description>&lt;p&gt;Let’s take a look at how Python’s typical data structures (lists, tuples, dicts, sets) are implemented, and how we can take advantage of this knowledge to optimize our code. The implementation we are focusing on is CPython.&lt;/p&gt;

&lt;p&gt;CPython is the &lt;a href=&quot;http://stackoverflow.com/questions/17130975/python-vs-cpython&quot;&gt;original implementation&lt;/a&gt; of Python, written in C. Since Python is a high-level language, the code you execute has to be evaluated and interpreted somehow. CPython is the program that translates Python instructions, written following the language specification, to something the machine can actually execute. Specifically, it compiles the code into bytecode, which then get interpreted and executed.
There are other implementations of Python “the language”, like &lt;a href=&quot;http://www.jython.org/&quot;&gt;Jython&lt;/a&gt; written in Java and &lt;a href=&quot;http://ironpython.net/&quot;&gt;Iron Python&lt;/a&gt; written in C#.
The high-level Python code you wrote is the same (a list of integers is &lt;code class=&quot;highlighter-rouge&quot;&gt;l = [1, 2, 3]&lt;/code&gt; in all of them), it is the underlying implementation that changes: for example CPython creates a C array, while Jython creates a Java ArrayList.&lt;/p&gt;

&lt;p&gt;Knowing how the language data structures are actually implemented can give you a deeper understanding of what’s going on, and can allow you to optimize your code.
Now we’ll see the implementation of lists, tuples, dictionaries and sets.&lt;/p&gt;

&lt;h3 id=&quot;lists&quot;&gt;Lists&lt;/h3&gt;
&lt;p&gt;Python’s lists are implemented as variable-length arrays of &lt;code class=&quot;highlighter-rouge&quot;&gt;PyObject&lt;/code&gt;s. The name is a bit confusing because it resembles Linked Lists, but they are actually arrays.&lt;/p&gt;

&lt;p&gt;The implementation uses a contiguous array of references to other objects, and keeps a pointer to this array and the array’s length in a list head structure.
This makes indexing a list an operation whose cost is independent of the size of the list or the value of the index.
When items are appended or inserted, the array of references is resized. Some cleverness is applied to improve the performance of appending items repeatedly; when the array must be grown, some extra space is allocated so the next few times don’t require an actual resize (otherwise, every append/insert would require a resize of the array resulting in very poor performances).
You can see this over-allocation at line 42 of the &lt;a href=&quot;https://hg.python.org/cpython/file/tip/Objects/listobject.c&quot;&gt;CPython source code&lt;/a&gt; of the list object. The over-allocation is enough to give linear time behavior over a long list of appends. The growth pattern is: 0, 4, 8, 16, 25, 46, 58, 72, 88, …
Thus, if you perform a million consecutive appends, the resizing mechanism accounts for this and doesn’t resize a lot of times.&lt;/p&gt;

&lt;p&gt;Python lists are optimized for fast fixed-length operations and incur O(n) memory movement costs for &lt;code class=&quot;highlighter-rouge&quot;&gt;pop(0)&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;insert(0, v)&lt;/code&gt; operations which change both the size and, more important, the position of the underlying data representation.&lt;/p&gt;

&lt;p&gt;The following two little experiments show some optimizations we can do:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def f(n):
    l = []
    for i in range(n):
        l.append(i)
    return l

def g(n):
    l = []
    for i in range(n):
        l.insert(i, i)
    return l
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The two functions has the same effect, they return a list of natural numbers from 0 to n. The function f is slightly faster, as you can see from this plot (the execution time was measured using the handy ipython module &lt;code class=&quot;highlighter-rouge&quot;&gt;%timeit&lt;/code&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../../img/functionsfg.png&quot; alt=&quot;plot of python functions execution time&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following experiment shows the (huge) O(n) cost for resizing of the &lt;code class=&quot;highlighter-rouge&quot;&gt;insert(0, v)&lt;/code&gt; operation. If we modify the functions above to return the list of natural numbers in inverted order, we can see that it is much (very much) faster to append the elements and then &lt;code class=&quot;highlighter-rouge&quot;&gt;reverse()&lt;/code&gt; the resulting list rather than doing &lt;code class=&quot;highlighter-rouge&quot;&gt;insert(0, v)&lt;/code&gt; operations.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def f(n):
    l = []
    for i in range(n):
        l.append(i)
    l.reverse()
    return l

def g(n):
    l = []
    for i in range(n):
        l.insert(0, i)
    return l
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The difference between the two is so big that instead of plotting the results I’ll show you the numbers (unit is the microsecond):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input:      [100,  1000, 1000, 50000, 100000, 200000]
function f: [9.43, 89.1, 849, 4430, 9300, 18900]
function g: [24.3, 463, 29300, 785000, 3240000, 13000000]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;tuples&quot;&gt;Tuples&lt;/h3&gt;
&lt;p&gt;The main difference between lists and tuples is that tuples are immutable objects: that is once they are created, they cannot be modified anymore (of course the tuple cannot be modified, but the variable pointing to it can).
Because of this immutability property, tuples are useful for representing what other languages often call records — some related information that belongs together, like a person’s information. A tuple lets us “chunk” together related information and use it as a single thing.&lt;/p&gt;

&lt;p&gt;Python’s tuple implementation is basically the same as list but with optimizations that exploit the particular features of tuples, like fixed size. A very clever (in my opinion) optimization is one that is employed with allocation of small tuples.
Taken from &lt;a href=&quot;http://stackoverflow.com/questions/14135542/how-is-tuple-implemented-in-cpython&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
To avoid allocating a bunch of small objects, CPython recycles memory for many small tuples. There is a fixed constant (PyTuple_MAXSAVESIZE) such that all tuples less than this length are eligible to have their space reclaimed. Whenever an object of length less than this constant is deallocated, there is a chance that the memory associated with it will not be freed and instead will be stored in a &quot;free list&quot; (more on that in the next paragraph) based on its size. That way, if you ever need to allocate a tuple of size n and one has previously been allocated and is no longer in use, CPython can just recycle the old array.
&lt;/blockquote&gt;

&lt;h3 id=&quot;dictionaries&quot;&gt;Dictionaries&lt;/h3&gt;
&lt;p&gt;Python dictionaries are implemented using hash tables. A hash table is an array whose indexes are obtained using a hash function on the keys. The key can be a number, a string or an object, the important thing is that it must be an immutable object (i.e. we can define a hash function on it). The hash function is a function &lt;code class=&quot;highlighter-rouge&quot;&gt;h : K -&amp;gt; N&lt;/code&gt; which takes in input a key &lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt; and return an integer &lt;code class=&quot;highlighter-rouge&quot;&gt;h(k)&lt;/code&gt; representing the array index of the value corresponding to the key.
The goal of a hash function is to distribute the keys evenly in the array. A good hash function minimizes the number of collisions e.g. different keys having the same hash.&lt;/p&gt;

&lt;p&gt;When there is a collision, it must be resolved. A naive approach would be to have linked lists of values instead of values in the values array, but then the lookup operation would not be O(1) anymore. Instead, Python uses a quadratic probing sequence to find a free slot in the case of collisions.&lt;/p&gt;

&lt;p&gt;So every time you want to verify if a dictionary contains a key, what the interpreter does is to hash the value of the key to obtain the index and then verify if array[index] is empty. We would want to minimize the number of such operations.&lt;/p&gt;

&lt;p&gt;For example, a common use case of dictionaries is counting object, like word frequencies. In these cases, items are initialized once and modified many times. To implement this function we could write something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def f(words):
    wdict = {}
    for word in words:
        if word not in wdict:
            wdict[word] = 0
        wdict[word] += 1
    return wdict
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The problem with the above code is that except for the first time a new word is seen, the if test will fail every time, wasting a lot of hash function computations. We can avoid this with the following code:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def g(words):
    wdict = {}
    for word in words:
        try:
            wdict[word] += 1
        except KeyError:
            wdict[word] = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Note: it is important to use KeyError in the except clause. You want the dictionary to be modified only when this exception occurs.&lt;/p&gt;

&lt;p&gt;If the list is small the two functions are absolutely interchangeable. When the list get reasonably large, the second approach gets usually faster. For example, with the list I tried I got &lt;code class=&quot;highlighter-rouge&quot;&gt;3.08s&lt;/code&gt; execution time with the first approach and &lt;code class=&quot;highlighter-rouge&quot;&gt;2.66s&lt;/code&gt; with the second one.&lt;/p&gt;

&lt;h3 id=&quot;sets&quot;&gt;Sets&lt;/h3&gt;
&lt;p&gt;The implementation of Python sets derive from the dictionary implementation. Indeed, there is some &lt;a href=&quot;http://markmail.org/message/ktzomp4uwrmnzao6&quot;&gt;evidence&lt;/a&gt; that originally the implementation was mostly a copy-paste from the dict implementation.&lt;/p&gt;

&lt;p&gt;Python sets are implemented as dictionaries with dummy values (i.e. the keys are the members of the set) and there are optimizations for cache locality which take advantage of this lack of values. Thus, sets membership is done in O(1) as in dictionaries. Note that in the worst case (i.e. all hash collisions) the membership-checking is O(n).&lt;/p&gt;

&lt;p&gt;Because of this amazingly fast membership checking, sets are the obvious choice when you have a list of values and you don’t care about the order of the items, but just whether an item belongs to the list or not. Of course this is true only if the set is already there. If you include the set creation time, then a list is usually faster, as the following experiment shows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def f(e):
    l = range(1000000)
    return e in l

def g(e):
    s = set(range(1000000))
    return e in l
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;f(645000)&lt;/code&gt; took &lt;code class=&quot;highlighter-rouge&quot;&gt;30.7ms&lt;/code&gt; while &lt;code class=&quot;highlighter-rouge&quot;&gt;g(645000)&lt;/code&gt; took &lt;code class=&quot;highlighter-rouge&quot;&gt;142ms&lt;/code&gt;. But if we measure only the membership-checking time, then things are different: the list took &lt;code class=&quot;highlighter-rouge&quot;&gt;13.9ms&lt;/code&gt; while the set only &lt;code class=&quot;highlighter-rouge&quot;&gt;65.4ns&lt;/code&gt;! So keep in mind that in order to take advantage of the set data structure you have to be in a situation when you create the set once and read it many times.&lt;/p&gt;

&lt;p&gt;That’s it for now. I hope you enjoyed the article. Please write a comment if you have any feedback.&lt;/p&gt;

&lt;p&gt;PS: for further reference, there’s no better place than the CPython source code: &lt;a href=&quot;https://hg.python.org/cpython/file/tip/Objects/listobject.c&quot;&gt;listobject source code&lt;/a&gt;, &lt;a href=&quot;https://hg.python.org/cpython/file/tip/Objects/dictobject.c&quot;&gt;dictobject&lt;/a&gt;, &lt;a href=&quot;https://hg.python.org/cpython/file/tip/Objects/tupleobject.c&quot;&gt;tupleobject&lt;/a&gt;, &lt;a href=&quot;https://hg.python.org/cpython/file/tip/Objects/setobject.c&quot;&gt;setobject&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Wed, 23 Mar 2016 19:30:20 +0100</pubDate>
        <link>http://www.gabrieleangeletti.com/blog/programming/2016/03/23/python-underlying-data-structures.html</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/blog/programming/2016/03/23/python-underlying-data-structures.html</guid>
        
        
        <category>blog</category>
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Visualizing Twitter data: a Python example</title>
        <description>&lt;p&gt;Data visualization is used to synthesize data, explain concepts and communicate results and insights in a clear and efficient way. And most important, it’s fun!
In this post we’ll see an example of data visualization using Twitter API.&lt;/p&gt;

&lt;p&gt;This post is realized in collaboration with a friend of mine, and university collegue
&lt;a href=&quot;https://it-it.facebook.com/people/Alessandro-Stagni/100007421145017&quot;&gt;Alessandro Stagni&lt;/a&gt;.
We used &lt;a href=&quot;http://ipython.org/notebook.html&quot;&gt;IPython Notebooks&lt;/a&gt; to present the work. Notebooks are a great way to show
Python work: text, code, output and even images can be mixed in one interactive document
and then converted to html. Some people are even &lt;a href=&quot;https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python&quot;&gt;writing books&lt;/a&gt; using ipython notebooks!&lt;/p&gt;

&lt;p&gt;Data is collected from Twitter using the &lt;a href=&quot;https://dev.twitter.com/streaming/overview&quot;&gt;streaming API&lt;/a&gt;.
We collected 250 thousand tweets for each of the following languages: english, french, german, spanish, portuguese.&lt;/p&gt;

&lt;p&gt;We show visualizations of length distribution for each language, and WordClouds for
the most frequent words.&lt;/p&gt;

&lt;p&gt;WordClouds are generated using &lt;a href=&quot;https://github.com/amueller/word_cloud&quot;&gt;this&lt;/a&gt; great package.&lt;/p&gt;

&lt;p&gt;This is the &lt;a href=&quot;http://www.gabrieleangeletti.com/tweet_data_viz.html&quot;&gt;link to the notebook&lt;/a&gt;, hope you enjoy it!&lt;/p&gt;

</description>
        <pubDate>Sat, 12 Mar 2016 21:30:20 +0100</pubDate>
        <link>http://www.gabrieleangeletti.com/blog/data-science/2016/03/12/twitter-data-visualization-python.html</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/blog/data-science/2016/03/12/twitter-data-visualization-python.html</guid>
        
        
        <category>blog</category>
        
        <category>data-science</category>
        
      </item>
    
      <item>
        <title>Denoising Autoencoders: Tutorial + TensorFlow implementation</title>
        <description>&lt;p&gt;Denoising Autoencoders are a special kind of Neural Network trained to extract meaningful and &lt;strong&gt;robust&lt;/strong&gt; features from the input data. They can be stacked to form Deep Neural Networks, and their performance is state-of-the-art in many popular benchmarks.&lt;/p&gt;

&lt;p&gt;This tutorial is an overview of autoencoders. First, the general model is described, then we’ll see what is a &lt;em&gt;denoising&lt;/em&gt; autoencoder (and why it works better) and how we can stack them to obtain Deep Networks.
The implementation of the algorithm is written in Python using &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt;, and you can find it at this &lt;a href=&quot;https://gist.github.com/blackecho/3a6e4d512d3aa8aa6cf9&quot;&gt;github gist&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: this tutorial is for people with at least some working knowledge of Neural Networks, and it is only intended as a brief overview of the topic. I recommend you to read the &lt;a href=&quot;http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf&quot;&gt;original paper&lt;/a&gt; for further reference.&lt;/p&gt;

&lt;h4 id=&quot;what-is-an-autoencoder&quot;&gt;What is an Autoencoder?&lt;/h4&gt;

&lt;p&gt;Put simply, an autoencoder is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;&gt;Feed Forward Neural Network&lt;/a&gt; trained to reconstruct its input. As usual, you have an input layer, a hidden layer, and an output layer. If you have used NNs for classification tasks, you may be used to the idea that the output layer has as many nodes as there are classes in the dataset. For example, if you have worked with &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST&lt;/a&gt; dataset (the hello world of machine learning), you saw that the output of the network was a 10 nodes (because MNIST has 10 classes) &lt;em&gt;softmax layer&lt;/em&gt; where the i-th node represents the probability that the input pattern belong to the i-th class.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../../img/autoencoder.png&quot; alt=&quot;neural networks architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Autoencoders are an unsupervised learning algorithm though, so labels are either not available or we don’t care about them. Autoencoders are trained to reconstruct their input, and in order to do this, it is clear that the input and the output layers must have the same dimensionality.&lt;/p&gt;

&lt;p&gt;Thus, an autoencoder learns two mappings: the first from the input to the hidden layer (we call this mapping the &lt;strong&gt;encoder&lt;/strong&gt;), and the second from the hidden to the output layer (this one is called the &lt;strong&gt;decoder&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;It’s easy to see why they are called encoder/decoder: the encoder maps data from the input space to the hidden space, and the decoder maps encoded data back to the original space. You can think of the hidden representation as a &lt;em&gt;code&lt;/em&gt;, from which the original data can be decoded back. Of course, the code must capture the main features in the data, otherwise it would be impossible to extract something meaningful from it.&lt;/p&gt;

&lt;h4 id=&quot;autoencoder-algorithm&quot;&gt;Autoencoder Algorithm&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: In the following notation $x$ is the input, $y$ is the encoded data, $z$ is the decoded data, $\sigma$ is a non-linear activation function (sigmoid or hyperbolic tangent usually), and $f(x ; \theta)$ means that $f$ is a function of $x$ parameterized by $\theta$.&lt;/p&gt;

&lt;p&gt;The model can be summarized in the following way:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h6 id=&quot;the-input-data-is-mapped-onto-the-hidden-layer-encoding-the-mapping-is-usually-an-affine-transformation--followed-by-a-non-linearity-something-like&quot;&gt;The input data is mapped onto the hidden layer (&lt;em&gt;encoding&lt;/em&gt;). The mapping is usually an affine transformation  followed by a non-linearity. Something like:&lt;/h6&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;y = f(x ; \theta) = \sigma(Wx + b)&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h6 id=&quot;the-hidden-layer-is-mapped-onto-the-output-layer-decoding-the-mapping-is-an-affine-transformation-optionally-followed-by-a-non-linearity-something-like&quot;&gt;The hidden layer is mapped onto the output layer (&lt;em&gt;decoding&lt;/em&gt;). The mapping is an affine transformation optionally followed by a non-linearity. Something like:&lt;/h6&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;z = g(y ; \theta^{&#39;}) = g(f(x ; \theta) ; \theta^{&#39;}) = \sigma(W^{&#39;}y + b^{&#39;})&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h6 id=&quot;optionally-in-order-to-reduce-the-size-of-the-model-one-can-use-tied-weights-that-is-the-decoder-weights-matrix-is-constrained-to-be-the-transpose-of-the-encoder-weights-matrix-theta--thetat&quot;&gt;Optionally, in order to reduce the size of the model, one can use &lt;em&gt;tied weights&lt;/em&gt;, that is the decoder weights matrix is constrained to be the transpose of the encoder weights matrix: $\theta^{‘} = \theta^{T}$.&lt;/h6&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h6 id=&quot;the-output-of-the-autoencoder-is-matched-against-the-original-input-using-a-specific-criterion-cross-entropy-loss-or-mean-squared-error-usually-something-like&quot;&gt;The output of the autoencoder is matched against the original input using a specific criterion (cross entropy loss or mean squared error usually). Something like:&lt;/h6&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;e = \frac{1}{2m} \sum_{i = 1}^{m}|| x_{i} - z_{i} ||^{2}&lt;/script&gt; for mse or
&lt;script type=&quot;math/tex&quot;&gt;e = - \frac{1}{m} \sum_{i = 1}^{m} x_{i} log(z_{i}) + (1 - x_{i})log(1 - z_{i})&lt;/script&gt; for ce.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h6 id=&quot;using-backpropagationbp-small-changes-are-made-to-the-parameters-of-the-model-to-make-the-reconstructions-more-similar-to-the-original-input-note-that-with-tensorflow-you-dont-have-to-explicitly-compute-the-gradients-the-backpropagation-algorithm-is-automatically-applied-to-your-model-using-one-of-the-available-optimizerstfopt&quot;&gt;Using &lt;a href=&quot;http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/&quot;&gt;Backpropagation&lt;/a&gt;, small changes are made to the parameters of the model to make the reconstructions more similar to the original input. Note that with TensorFlow you don’t have to explicitly compute the gradients. The Backpropagation algorithm is automatically applied to your model using one of the available &lt;a href=&quot;https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#optimizers&quot;&gt;optimizers&lt;/a&gt;.&lt;/h6&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;notes-on-different-architectures&quot;&gt;Notes on different Architectures&lt;/h4&gt;

&lt;p&gt;The hidden layer can have either a lower or a higher dimensionality than that of the input/output layers.&lt;/p&gt;

&lt;p&gt;In the former case, the decoder reconstructs the original input from a lower-dimensional representation of it (we call it an &lt;strong&gt;under-complete&lt;/strong&gt; representation). For the overall thing to work, the encoder should learn to provide a low-dimensional representation that captures the essence of the data (i.e. the main factors of variations in the distribution). Intuitively, if the data was a five minute speach, the encoder would have to describe it in one minute, in other words it is forced to find a good way to summarize the data.&lt;/p&gt;

&lt;p&gt;In the latter case, the decoder reconstruction is build from a higher-dimensional representation (we call this one an &lt;strong&gt;over-complete&lt;/strong&gt; representation). You might think that these kind of representations are kind of useless. If the input dimension is 500, and the hidden layer dimension is 700, there is a great chance that the autoencoder will use 500 of its 700 hidden units to learn the identity mapping, and “throw away” the others.
Although this can happen, it turns out that if we constrain the hidden units so that only a fraction of them are active at any given time, it can act as a strong regularizer making the autoencoder work surprisingly well. These are the so called &lt;a href=&quot;https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf&quot;&gt;Sparse Autoencoders&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;denoising-autoencoders&quot;&gt;Denoising Autoencoders&lt;/h4&gt;

&lt;p&gt;So what are Denoising autoencoders? They are very similar to the model we saw previously, the difference is that in this case the input is &lt;strong&gt;corrupted&lt;/strong&gt; before being passed to the network. The trick is that by matching the original version (not the corrupted one) with the reconstruction at error computing time, the autoencoder is trained to reconstruct the &lt;em&gt;original input&lt;/em&gt; from the corrupted version. An approximate schema is the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h6 id=&quot;the-input-x-gets-corrupted-by-a-function-qx--xcorr&quot;&gt;the input $x$ gets corrupted by a function $q(x) = x_{corr}$.&lt;/h6&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h6 id=&quot;xcorr-is-used-as-in-the-previous-model&quot;&gt;$x_{corr}$ is used as in the previous model:&lt;/h6&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;h6 id=&quot;y--fxcorr--theta--sigmawxcorr--b-&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;y = f(x_{corr} ; \theta) = \sigma(Wx_{corr} + b)&lt;/script&gt;&lt;/h6&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;h6 id=&quot;z--gy--theta--gfxcorr--theta--theta--sigmawy--b-&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;z = g(y ; \theta^{&#39;}) = g(f(x_{corr} ; \theta) ; \theta^{&#39;}) = \sigma(W^{&#39;}y + b^{&#39;})&lt;/script&gt;&lt;/h6&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h6 id=&quot;the-error-computation-is-exactly-the-same-as-the-previous-model-use-the-original-x&quot;&gt;The error computation is exactly the same as the previous model (use the original x)&lt;/h6&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Intuitively, if we observe an autoencoder capable of taking in input a distorted version of something and returning a good reconstruction of it, it means that it was very clever about the representation it has learned, otherwise it wouldn’t be able to reconstruct the data.
To get a sense of what corrupt the input means, below is an image taken from the MNIST dataset, you can clearly see which is the original and which is the corrupted version.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../../img/8.png&quot; width=&quot;100px !important&quot; alt=&quot;image of an eight&quot; /&gt;
&lt;img src=&quot;../../../../../img/8c.png&quot; alt=&quot;image of a distorted eight&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The corruption technique used in this image is the &lt;strong&gt;masking noise&lt;/strong&gt; technique, that is a fraction of the input elements taken at random is set to zero. In the image above, the fraction was 50% of the pixels.
The other techniques discussed in the paper are the &lt;strong&gt;salt and pepper noise&lt;/strong&gt;, where a fraction of the input elements taken at random is set either to the maximum value or to the minimum, and the &lt;strong&gt;white gaussian noise&lt;/strong&gt;. The authors emphasize the fact that they used these methods because they are very general and can be applied to a wide variety of data. Of course, if you know well the domain at hand (e.g. images) and use a technique to better expose the main structure in your data, the algorithm is expected to work better.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The corruption process has to be repeated each time the same input is presented to the autoencoder. If the training set is corrupted only once at the beginning of training, the model would recognize the distortions as valid data patterns, leading to &lt;strong&gt;overfitting&lt;/strong&gt;. By corrupting the data every time in a different way instead, the model will learn to extract the underlying structure, avoiding overfitting.&lt;/p&gt;

&lt;h4 id=&quot;stacked-denoising-autoencoders&quot;&gt;Stacked Denoising Autoencoders&lt;/h4&gt;

&lt;p&gt;We can stack autoencoders in order to obtain Deep Neural Networks. The stacking process is done in the following way for a classification task:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h6 id=&quot;an-autoencoder-denoising-or-not-is-unsupervisely-trained-on-the-input-data-after-training-we-drop-the-decoder-and-we-encode-the-training-set-encoded-training-set--sigmaw1x--b1&quot;&gt;An Autoencoder (denoising or not) is “unsupervisely” trained on the input data. After training, we drop the decoder, and we encode the training set: encoded training set = $\sigma(W_{1}x + b_{1})$.&lt;/h6&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h6 id=&quot;the-encoded-data-is-used-as-training-set-for-a-second-autoencoder-after-training-we-drop-the-decoder-and-we-encode-the-encoded-training-set-sigmaw2sigmaw1x--b1--b2&quot;&gt;The encoded data is used as training set for a second Autoencoder. After training, we drop the decoder, and we encode the encoded training set: $\sigma(W_{2}\sigma(W_{1}x + b_{1}) + b_{2})$&lt;/h6&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h6 id=&quot;we-do-this-for-as-many-layers-we-want-at-the-end-we-attach-a-softmax-layer-on-top-of-the-last-autoencoders-encoder-and-we-train-the-whole-thing-as-if-it-was-a-simple-feed-forward-neural-network-using-backpropagation-of-course-the-initial-weights-of-the-neural-net-must-be-initialized-at-the-value-that-the-autoencoders-had-after-training-otherwise-the-pretraining-procedure-would-be-useless&quot;&gt;We do this for as many layers we want. At the end, we attach a softmax layer on top of the last autoencoder’s encoder, and we train the whole thing as if it was a simple Feed Forward Neural Network using Backpropagation (of course the initial weights of the Neural Net must be initialized at the value that the autoencoders had after training, otherwise the pretraining procedure would be useless).&lt;/h6&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&quot;https://gist.github.com/blackecho/3a6e4d512d3aa8aa6cf9&quot;&gt;github gist&lt;/a&gt; contains only an implementation of a Denoising Autoencoder. If you want to see a working implementation of a Stacked Autoencoder, as well as many other Deep Learning algorithms, I encourage you to take a look at &lt;a href=&quot;https://github.com/blackecho/Deep-Learning-TensorFlow&quot;&gt;my repository&lt;/a&gt; of Deep Learning algorithms implemented in TensorFlow.&lt;/p&gt;

</description>
        <pubDate>Mon, 29 Feb 2016 19:30:20 +0100</pubDate>
        <link>http://www.gabrieleangeletti.com/blog/machine-learning/2016/02/29/denoising-autoencoder-tensorflow.html</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/blog/machine-learning/2016/02/29/denoising-autoencoder-tensorflow.html</guid>
        
        
        <category>blog</category>
        
        <category>machine-learning</category>
        
      </item>
    
      <item>
        <title>Refactoring a Restricted Boltzmann Machine implementation in TensorFlow.</title>
        <description>&lt;p&gt;I’m currently reading the excellent book &lt;a href=&quot;http://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882&quot;&gt;Clean Code&lt;/a&gt;, a book about writing code in a professional way, and I applied the lessons learned in this book to my implementation of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine&quot;&gt;Restricted Boltzmann Machine&lt;/a&gt; using &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TensorFlow is an open source library developed at Google for distributed computation of machine learning algorithms. I’ll write another post about it, for now let’s focus only on refactoring the code. You don’t have to be a TensorFlow expert in order to understand the refactoring, it’s just about modifying names, removing comments and rearranging functions.&lt;/p&gt;

&lt;p&gt;The old code (before the refactoring) and the new code (after the refactoring) can be found at this &lt;a href=&quot;https://gist.github.com/blackecho/db85fab069bd2d6fb3e7&quot;&gt;github gist&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Clean code is an amazing book full of lessons about writing &lt;strong&gt;good&lt;/strong&gt; code. Indeed I learned a lot of lessons since I realized my coding style was “not so good” after I saw how clean code can be &lt;strong&gt;elegant&lt;/strong&gt; and pleasant to the eyes. Since I haven’t finished the book yet, I want about the three things I learned the most about: naming things, functions and comments.&lt;/p&gt;

&lt;h3 id=&quot;naming&quot;&gt;Naming&lt;/h3&gt;

&lt;p&gt;This quote from the book can’t explain better the concept:&lt;/p&gt;
&lt;blockquote&gt;
Names should reveal intent. A name should tell you why the variable, class, etc.. exits, what it does, and how it is used. If a name requires a comment, then the name does not reveal its intent.
&lt;/blockquote&gt;
&lt;p&gt;For example, you can see in the old code that the instance variables corresponding to the number of visible/hidden units in the rbm are called respectively &lt;code class=&quot;highlighter-rouge&quot;&gt;nvis/nhid&lt;/code&gt;.
While the meaning of these variables can be deducted from the context of the class and their usage, the name isn’t much self-explaining, probably a comment would be needed to make the reader instantly understand the meaning of them. Instead, &lt;code class=&quot;highlighter-rouge&quot;&gt;num_visible/num_hidden&lt;/code&gt; are names that can’t be misunderstood. As you read them, you immediately understand what they are, why they are there, and how they are used.&lt;/p&gt;

&lt;h3 id=&quot;functions&quot;&gt;Functions&lt;/h3&gt;

&lt;p&gt;Functions should do one thing. A quote from the book:&lt;/p&gt;
&lt;blockquote&gt;
The first rule of functions is that they should be small. The second rule of functions is that they should be smaller than that.
&lt;/blockquote&gt;
&lt;p&gt;This is the single most valuable thing I learned from this book. Clean code is mainly about &lt;strong&gt;readability&lt;/strong&gt;, &lt;strong&gt;maintainability&lt;/strong&gt; and &lt;strong&gt;elegance&lt;/strong&gt;. And I think that writing functions that do one thing is one of the most effective ways to achieve cleanness.&lt;/p&gt;

&lt;p&gt;For example, look at the &lt;code class=&quot;highlighter-rouge&quot;&gt;_create_graph&lt;/code&gt; method (&lt;code class=&quot;highlighter-rouge&quot;&gt;_build_model&lt;/code&gt; in the new code). This function is way too long and it does a lot of different things like creating placeholders, variables, performing gibbs sampling, computing positive and negative associations, compute the update rules and the loss function. This is really a good example of a function that does way more than one thing!
In the new code, this single function is replaced by seven new functions, one for each thing the old function did. The resulting code is much cleaner.&lt;/p&gt;

&lt;p&gt;Another thing about functions is the order in which they should be placed in the source file.
In particural, the caller should be above the callee. The author of the book explains this rule in terms of the &lt;em&gt;newspaper&lt;/em&gt; metaphor: the source file should look like a newspaper article. At the top there is the title, and a short outline of what its told in the article. Then it begins explaining the broad concepts of the story, and then below are all the details of who says what, when things happened etc..&lt;/p&gt;
&lt;blockquote&gt;
Master Programmers think of systems as stories to be told rather than programs to be written. With this goal in mind, functions need to fit cleanly together into a clear and precise language to help you with that telling.
&lt;/blockquote&gt;
&lt;p&gt;In the new code, the &lt;code class=&quot;highlighter-rouge&quot;&gt;_build_model&lt;/code&gt; function is splitted in seven new functions in a top-down fashion. To build a model, the first thing you have to do is to create placeholders (TensorFlow structure for the data to be passed to the algorithm). And indeed, &lt;code class=&quot;highlighter-rouge&quot;&gt;_create_placeholders&lt;/code&gt; is the first function below &lt;code class=&quot;highlighter-rouge&quot;&gt;_build_model&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then, the second thing you have to do is to create variables (TensorFlow structure to store the parameters of the model), in this case the weights and biases of the Restricted Boltzmann Machine. Again, you can see that &lt;code class=&quot;highlighter-rouge&quot;&gt;_create_variables&lt;/code&gt; is below &lt;code class=&quot;highlighter-rouge&quot;&gt;_create_placeholders&lt;/code&gt;, and so on.&lt;/p&gt;

&lt;p&gt;The code is much cleaner. At first you are presented with the &lt;code class=&quot;highlighter-rouge&quot;&gt;_build_model&lt;/code&gt; function, which tells you the broad concept of constructing a RBM model using TensorFlow. As you scroll down more details are revealed to you: how do I create a placeholder? A variable? How is gibbs sampling implemented? As you dive deeper in the story, the details are revealed to you.&lt;/p&gt;

&lt;h3 id=&quot;comments&quot;&gt;Comments&lt;/h3&gt;

&lt;p&gt;By reading this book I realized that my idea about comments was completely wrong. I’ve always thought that comments are always a good thing, and its always better to put a comment than to put nothing. Why should comments be a bad thing? They help us expressing ourselves better!&lt;/p&gt;

&lt;p&gt;And this is the point made by the author: &lt;strong&gt;we use comments because we fail to express ourselves in code&lt;/strong&gt;.
Bad comments are far worse than no comments at all! Bad comments can even lie about the code. Often also good comments can lie about the code. This is because is very difficult for programmers to maintain them in a consistent state as the code evolves. Code changes very often, and if comments do not change consistently with the code, we can find a comment in another place than it was supposed to be, or a good comment that now is wrong because of a change in the code, and so on.&lt;/p&gt;

&lt;p&gt;Comments should be kept at a minimum, and should be used only when we can’t find a way to express the concept in code. When you are in a position in which you have to write a comment, think about it first and see if you can came up with a way to express yourself in code rather than with that comment.
In the old code you can find a lot of useless comments, like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Randomly shuffle the input
np.random.shuffle(trX)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You already know (or at least imagine) what the &lt;a href=&quot;http://www.numpy.org/&quot;&gt;numpy&lt;/a&gt; shuffle function does. This comment is completely redundant. As you can tell, there are a lot of examples like this in the old code.&lt;/p&gt;

&lt;p&gt;This was just a small example of refactoring and writing clean code. I’ll write more about the topic and my experiments/lessons learned. I encourage you to take a look at the &lt;a href=&quot;https://gist.github.com/blackecho/db85fab069bd2d6fb3e7&quot;&gt;code&lt;/a&gt;, and please comment below if you have any suggestion/ideas on how to improve it.&lt;/p&gt;

</description>
        <pubDate>Sun, 21 Feb 2016 15:30:20 +0100</pubDate>
        <link>http://www.gabrieleangeletti.com/blog/programming/2016/02/21/refactoring-rbm-tensor-flow-implementation.html</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/blog/programming/2016/02/21/refactoring-rbm-tensor-flow-implementation.html</guid>
        
        
        <category>blog</category>
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Hierarchical Reinforcement Learning</title>
        <description>&lt;p&gt;Project on Hierarchical RL. Implementation of the MAX-QQ value function decomposition and algorithm.
Link to &lt;a href=&quot;https://github.com/blackecho/Hierarchical-Reinforcement-Learning&quot;&gt;github&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 10 Sep 2015 16:30:20 +0200</pubDate>
        <link>http://www.gabrieleangeletti.com/projects/deep-belief-nets</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/projects/deep-belief-nets</guid>
        
        
        <category>projects</category>
        
        <category>project</category>
        
      </item>
    
      <item>
        <title>Deep Belief Networks</title>
        <description>&lt;p&gt;Theoretical study and implementation of Restricted Boltzmann Machine and Deep Belief Networks.
Link to &lt;a href=&quot;http://github.com/blackecho/Deep-Belief-Network&quot;&gt;github&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 10 Sep 2015 16:30:20 +0200</pubDate>
        <link>http://www.gabrieleangeletti.com/projects/deep-belief-nets</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/projects/deep-belief-nets</guid>
        
        
        <category>projects</category>
        
        <category>project</category>
        
      </item>
    
      <item>
        <title>Sparse Distributed Representations in Machine Learning</title>
        <description>&lt;p&gt;Today we will take a look at &lt;a href=&quot;https://github.com/numenta/nupic/wiki/Sparse-Distributed-Representations&quot;&gt;Sparse Distributed Representations&lt;/a&gt;: what they are, why they differ from the standard types of representations used by computers, and why many people think they are fundamental in building true machine intelligence.&lt;/p&gt;

&lt;p&gt;Computers manipulate information through so called &lt;strong&gt;dense&lt;/strong&gt; representations: basically, they use a small number of bits and every combination of them represent something. For example, think about &lt;a href=&quot;https://en.wikipedia.org/wiki/ASCII&quot;&gt;ASCII&lt;/a&gt; code. It uses a fixed number of bits, and any combination of those bits represents something, for instance ‘110 0111’ is ‘g’.&lt;/p&gt;

&lt;p&gt;There are two things that make us thing this kind of representations are not suitable for machine learning applications.
First, the individual bits has no meaning at all. They become meaningful only when they are put together. For example, pick the code corresponding to ‘g’ in ASCII. Someone has chosen that combination of bits to represent a ‘g’, but the fact that the third bit is 0 says anything about the concept of ‘g’.
Second, this kind of representations is not fault tolerant: if you change only one bit, you obtain a completely different meaning.&lt;/p&gt;

&lt;p&gt;Neuroscientists have discovered that this is not how the human brain works. Instead, the brain uses a Sparse Distributed Representation of information.&lt;/p&gt;

&lt;p&gt;You can think of a binary string as a set of neurons, where each neuron can be active (1) or not active (0). These representations are &lt;strong&gt;sparse&lt;/strong&gt; in the sense that at any given time, only a tiny fraction of the neurons are active, so you always have a very small percentage of 1s. They are also &lt;strong&gt;distributed&lt;/strong&gt; in the sense that they are a many-to-many mapping between concepts and neurons: each concept is represented by many neurons, and each neuron participates in the representation of many concepts. Here individual neurons have semantic meaning.&lt;/p&gt;

&lt;p&gt;For example, if you want to represent the concept of a car with a distributed representation, you can use a binary vector of length 1000, where each element represent the presence or absence of the feature it represent. In this sense neurons have semantic meaning: if the neuron in position 49 is active, you know that that car is red, regardless of all the other neurons.&lt;/p&gt;

&lt;p&gt;In ASCII code, the fact that the second bit is 1 is almost irrelevant. Instead, in a distributed representation, this is meaningful, in fact you can say that two sequences are semantically similar if they share some indexes of active bits, the more they share the more they are similar. So you can even sample a subset of the bits and compare only those samples to infer similarity (this is one of the advantages of a distributed representation). They have another advantage: because the representation is sparse, you can store only the indices of the active bits, and because there a few active bits at a given time, you can save a lot of space!&lt;/p&gt;

&lt;p&gt;Sparse distributed representations are used by many algorithms: they are at the core of a new
brach of Artificial Intelligence algorithms, &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;Deep Learning&lt;/a&gt;, that is having huge success in the field, beating previous state-of-the-art algorithms in almost all branches, from image to audio to text.
There are many kind of Deep Learning Neural Networks, but all of them are based on the concept of
unsupervised pretraining: you train one layer at a time in an unsupervised fashion, and the output of one layer is used as training set for the next layer in the hierarchy. All these layers learn to detect &lt;strong&gt;features&lt;/strong&gt; of the training data, the higher the layer in the hierarchy, the more abstract and meaningful the features are expected to be. For instance, if we are recognizing faces in images, in the first layer one would expect to see edges, strokes, blobs, while in the second layer one would expect to see something like an eye, a mouth, and so no.
All these layers are indeed learning Sparse Distributed Representations of the training set, and this is way this is useful to know and study this topic in order to grasp the concepts behind Deep Learning.&lt;/p&gt;

</description>
        <pubDate>Mon, 15 Jun 2015 16:30:20 +0200</pubDate>
        <link>http://www.gabrieleangeletti.com/blog/machine-learning/2015/06/15/sparse-distributed-representations.html</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/blog/machine-learning/2015/06/15/sparse-distributed-representations.html</guid>
        
        
        <category>blog</category>
        
        <category>machine-learning</category>
        
      </item>
    
      <item>
        <title>The Forward-Backward Algorithm for Hidden Markov Models</title>
        <description>&lt;p&gt;Today we will see how we can do inference in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_Model&quot;&gt;Hidden Markov Model (HMM)&lt;/a&gt;, using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm&quot;&gt;Forward-Backward (FB) algorithm&lt;/a&gt;. First of all I will briefly recap what a HMM is, then I’ll show you the algorithm in detail.&lt;/p&gt;

&lt;p&gt;A HMM is a statistical model that can be used to do a lot of interesting things, it has many applications in speech-recognition, handwriting recognition, and many others.
The basic idea is that if we have some observations about something, they can be explained in terms of some states (which are hidden) to which these observations depend. If we take for example handwriting recognition, the hidden states would be the letters of the alphabet, and the observed variables would be the letters that one actually writes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../../img/hmm.png&quot; alt=&quot;hidden markov model graphic representation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The arrows in the image indicate a conditional probability, so there are conditional probabilities between states and between
states and observations.
These two quantities themselves, together with the initial distributions for the states, entirely define an HMM.
So you have two matrices, the transition probability matrix $T$ (or transition model) and the observation probability matrix $O$
(or observation model).&lt;/p&gt;

&lt;p&gt;$T$ contains the probabilities of changing state: if the model has N states, then the matrix will have dimensions NxN, in which the $T_{(i,j)}$ cell contains the probability that the next state will be j, given that the current state is i.&lt;/p&gt;

&lt;p&gt;$O$ contains the probabilities of observing a particular value, given the current state: for example the $O_{(i,j)}$ cell
contains the probability of observing value i given that the current state is j.&lt;/p&gt;

&lt;p&gt;When you have defined a HMM, you can compute the posterior probability distribution of the states given the observations, thus estimating the
correlations between the observed variables and the hidden states. In the handwriting example this means that we would be able to recognize the character
that one has written given the actual messy character that she/he wrote.&lt;/p&gt;

&lt;p&gt;The FB algorithm allows you to compute these posterior probabilities for internal hidden states.&lt;/p&gt;

&lt;p&gt;The basic idea is that this probability is proportional to the joint distribution $ P(Z_k,X_{1:t}) $ and, using &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayes&#39;_theorem&quot;&gt;Bayes theorem&lt;/a&gt; and the independence of $ X_{1:k} $ and $ X_{k+1:t} $ given $ Z_k $, we obtain:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Z_k | X_{1:t}) \propto P(Z_k , X_{1:t}) = P(Z_k | X_{1:k}) P(X_{k+1:t} | Z_k)&lt;/script&gt;

&lt;p&gt;This algorithm rely on the idea of &lt;a href=&quot;http://en.wikipedia.org/wiki/Dynamic_programming&quot;&gt;Dynamic Programming&lt;/a&gt;, that is a fancy name for saying that we reuse things we have already computed to make things faster.&lt;/p&gt;

&lt;p&gt;The forward part computes the posterior probability of state $Z_k$ given the first k observations $ X_{1:k}$, and does
this $\forall k = 1 … n$.&lt;/p&gt;

&lt;p&gt;The backward part computes the probability of the remaining observations $X_{k+1:n}$ given state $Z_k$, again for each k.&lt;/p&gt;

&lt;p&gt;The last part of the algorithm is to merge the forward and the backward part to give the full posterior probabilities.&lt;/p&gt;

&lt;p&gt;This algorithm gives you probabilities for all hidden states, but it doesn’t give probabilities for sequences of states (see the &lt;a href=&quot;http://en.wikipedia.org/wiki/Viterbi_algorithm&quot;&gt;Viterbi Algorithm&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;forward-part&quot;&gt;Forward Part&lt;/h2&gt;

&lt;p&gt;We want to use dynamic programming, so we want to express $P(Z_k | X_{1:k})$ as a function of k-1, so that we can setup a recursion. We achieve that by using &lt;a href=&quot;http://en.wikipedia.org/wiki/Marginal_distribution&quot;&gt;marginalization&lt;/a&gt;.
So using marginalization we can bring $ Z_{k-1} $ to the equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Z_k,X_{1:k}) = \sum_{Z_{k-1}} P(Z_k, Z_{k-1}, X_{1:k-1}, X_k)&lt;/script&gt;

&lt;p&gt;Where the sum is over all the values which $ Z_{k-1} $ can assume. Now we can use the &lt;a href=&quot;http://en.wikipedia.org/wiki/Chain_rule_(probability)&quot;&gt;chain rule for probability&lt;/a&gt;&amp;lt;/a&amp;gt; and see what happens:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{Z_{k-1}} P(Z_k, Z_{k-1}, X_{1:k-1}, X_k) = \sum_{Z_{k-1}} P(X_k | Z_k, Z_{k-1}, X_{1:k-1}) P(Z_k|Z_{k-1}, X_{1:k-1}) P(Z_{k-1} | X_{1:k-1}) P(X_{1:k-1})&lt;/script&gt;

&lt;p&gt;the last two terms can be unified in: $P(Z_{k-1}, X_{1:k-1})$. So now we use &lt;a href=&quot;http://www.andrew.cmu.edu/user/scheines/tutor/d-sep.html&quot;&gt;D-separation&lt;/a&gt; to simplify the above equation. In the first term we can remove $ Z_{k-1}$ and $ X_{1:k-1} $, because $ X_k $ is independent of them, given $ Z_k $. We can see it graphically using D-separation: the idea is that any path in the graph from $ X_k $ to $ Z_{k-1}$ or $ X_{1:k-1} $, must pass through $ Z_k $, so we say that they are independent given $ Z_k $ and we remove them.
The same goes for the second term, in which we remove $ X_{1:k-1} $. Let’s now write the simplified equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Z_k,X_{1:k}) = \sum_{Z_{k-1}} P(X_k | Z_k) P(Z_k|Z_{k-1}) P(Z_{k-1}, X_{1:k-1})&lt;/script&gt;

&lt;p&gt;Let’s examine this equation. The first term is the emission probability, which we have assumed we know. The second term is the transition probability, which also we have assumed we know. The third term is the recursion term over k-1, so we compute this value $\forall k = 2 … n$ and we are done.
For k = 1, the value is simple to compute:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Z_1,X_1) = P(X_1 | Z_1) P(Z_1)&lt;/script&gt;

&lt;h2 id=&quot;backward-part&quot;&gt;Backward Part&lt;/h2&gt;

&lt;p&gt;For the backward part, we compute the values in a similar way, by using marginalization and the chain rule.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X_{k+1, t}, Z_k) = \sum_{Z_{k+1}} P(X_{k+1:t}, Z_{k+1} | Z_k) = \sum_{Z_{k+1}} P(X_{k+1}, X_{k+2:t}, Z_{k+1} | Z_k)&lt;/script&gt;

&lt;p&gt;Now we again use the chain rule and see what happens:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{Z_{k+1}} = P(X_{k+2:t} | Z_{k+1}, X_{k+1}, Z_k) P(X_{k+1} | Z_{k+1}, Z_k) P(Z_{k+1}, Z_k)&lt;/script&gt;

&lt;p&gt;Like in the forward part, we can use D-separation to simplify the equation. In the first term, we can remove $ X_{k+1}$ and $Z_k$, and in the second
term we can remove $Z_k$. So now the equation is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{Z_{k+1}} = P(X_{k+2:t} | Z_{k+1}) P(X_{k+1} | Z_{k+1}) P(Z_{k+1}, Z_k)&lt;/script&gt;

&lt;p&gt;And again: we have an emission probability (second term), a transition probability (third term), and a recursion term over k+1 (first term).
We again compute this value $\forall k = 1 … n-1$ and we are done. For k=n, the value is 1 (you can compute it yourself is pretty easy).&lt;/p&gt;

</description>
        <pubDate>Sat, 30 May 2015 17:30:20 +0200</pubDate>
        <link>http://www.gabrieleangeletti.com/blog/machine-learning/2015/05/30/forward-backward-algorithm-hidden-markov-models.html</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/blog/machine-learning/2015/05/30/forward-backward-algorithm-hidden-markov-models.html</guid>
        
        
        <category>blog</category>
        
        <category>machine-learning</category>
        
      </item>
    
      <item>
        <title>A Breadth-First Search (BFS) Prolog meta-interpreter</title>
        <description>&lt;p&gt;Today we will build a meta-interpreter for Prolog that uses a Breadth-First search strategy. The Prolog built-in interpreter uses DFS by default to build the tree of computations.
This is due to performance reasons,&lt;/p&gt;

&lt;p&gt;because other searching policies wouldn’t be feasible, except for fairly easy problems.
But a cool fact about Prolog is that code and data are basically the same thing, and because of that its easy to implement a Prolog program that runs other Prolog programs (i.e. a meta-interpreter).
We can build meta-interpreters to change the behavior of our interpreter: we can for example change the search policy, do the occur check, return the depth of the computations tree, and many other things.&lt;/p&gt;

&lt;p&gt;In this post we will change Prolog’s search strategy from DFS to BFS, as an example of how we can easily change its behavior.
The main difference between DFS and BFS is that DFS uses a Stack to mantain the list of nodes to visit, and it means that DFS always tries to expand the last node it has visited. This policy can turn into trouble if there is a
left recursion, which is when a variable that appears in the head of a rule also appears in the first goal of the tail. Since Prolog doesn’t do the occur check, which is needed exactly to avoid such situations, it loops forever, until it goes out of local stack.&lt;/p&gt;

&lt;p&gt;With Breadth-First Search such situations can be avoided, because BFS will expand all the goals in a clause, instead of keep trying to expand the first node it encounter.
This is due do the fact that BFS uses a Queue to mantain the list of nodes to visit, so it when it expands a node, it won’t visit what it has just expanded, but it will continue to visit the other nodes it hasn’t expanded yet.&lt;/p&gt;

&lt;p&gt;So at each depth level, BFS will first visit all the nodes at that level, and then move to the next level. While this is bad for performance, because we don’t want any useless path to be visited, it can always find a solution if it exists, thus BFS is sound and complete. Let’s now implement the meta-interpreter.&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p&gt;We will use Prolog’s built-in predicate clause/2.
clause(Goal, Body) unifies Body with the bodies that matches with Goal, and return them in the form (FirstGoal, OtherGoals), where OtherGoals can be another list of the form (FirstGoal, OtherGoals). So clause/2 will find for us all the matches with Goal. Now we need a predicate that converts goal lists of this form into a standard prolog list with square brackets. To do this we will implement the predicate conj_goals_2_list.
Now we are ready to implement the meta-interpreter. Basically BFS is the same as DFS except that it uses a Queue instead of a Stack. The Queue works with a FIFO (first-in first-out) policy, so we always insert element at the last position of the list, and we always retrieve elements at the first position.
In Prolog we can do this pretty easily by using the way it handles lists and append/3.
Using the predicate append(OldList, Element, NewList), NewList will be the result of adding Element at the end of OldList.
So the code looks like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;solveBFS([Goal|Rest]) :- % list of goals to prove
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;we use clause/2 on the first element of the goals list, that is the first-out part of FIFO&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;clause(Goal, Body)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;we convert the list format returned by clause into a standart Prolog list&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conj_goals_2_list(Body, List)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;we append List to the end of Rest, that is the part first-in part of FIFO&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;append(Rest, List, New)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;we call recursively the solver passing it the new list. The program will keep trying to prove the goals, until all the goals are proved, or they can’t be proved&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;solveBFS(New)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The full code is available at &lt;a href=&quot;http://github.com/blackecho/prolog-programs&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Thu, 21 May 2015 16:30:20 +0200</pubDate>
        <link>http://www.gabrieleangeletti.com/blog/prolog/2015/05/21/prolog-bfs-meta-interpreter.html</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/blog/prolog/2015/05/21/prolog-bfs-meta-interpreter.html</guid>
        
        
        <category>blog</category>
        
        <category>prolog</category>
        
      </item>
    
      <item>
        <title>Solving Sudoku as a Constraint Satisfaction Problem (CSP) using Prolog</title>
        <description>&lt;p&gt;Today we will model the game of &lt;a href=&quot;http://en.wikipedia.org/wiki/Sudoku&quot;&gt;Sudoku&lt;/a&gt; as a &lt;a href=&quot;http://en.wikipedia.org/wiki/Constraint_satisfaction_problem&quot;&gt;Constraint Satisfaction Problem (CSP)&lt;/a&gt; and then write a Prolog program that solves it.
CSP is particularly useful when we want to model a combinatorial problem subject to &lt;strong&gt;constraints&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In standard Sudoku we have a 9x9 grid, where each cell can be filled with an integer between 1 and 9, so the possible configurations are all the ordered combinations of 81 numbers between 1 and 9.&lt;/p&gt;

&lt;p&gt;In the first cell, we have 9 different values that we can put in.
If we consider both the first and the second cells, we have $ 9^{2} $ different values that we can put in, because we have to take into account all the possible combination of
the form $ (1,1), (1,2) … (1,9), (2,1) … (9,9) $. So if we consider all the cells in the end we have $ 9^{81} $ possible combination of values, a very huge number!!&lt;/p&gt;

&lt;p&gt;But, two things help us here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sudoku is not about trying some random combination, it has some rules, and those rules greatly reduce the above number, because they tell us which combinations are right, and which are wrong.&lt;/li&gt;
  &lt;li&gt;Sudoku has some cells that are already filled, and these &lt;strong&gt;initial conditions&lt;/strong&gt; also help in reducing the huge number of tries.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So let’s see how we can model this game in csp.&lt;/p&gt;

&lt;p&gt;CSP has variables (what we are interested in), domains (which values the variables can assume) and constraints (rules about what values are right and what are wrong).
The variables in this case are the 81 cells of the grid, each of which can assume a value between 1 and 9, so the domain here is the integer interval [1, 9].
The constraints simply are the rules of the game, that are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All the values in each row must be different each other&lt;/li&gt;
  &lt;li&gt;All the values in each column must be different each other&lt;/li&gt;
  &lt;li&gt;All the values in each 3x3 block must be different each other&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Done! We don’t need some fancy algorithm or complex trick to get the job done: we are simply describing the problem here, and we let Prolog’s backtracking solve it for us. This is the true power of Prolog and logic
Programming: you don’t actually have to solve the problem, you only have to model it!&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p&gt;Prolog has a csp library, named &lt;strong&gt;clpfd&lt;/strong&gt;. It has all the things that we need: we can define variables, domains for the variables, and costraints. More over, the constraints that the values must be different, is a type of constraint that is already implemented in clpfd. All we have to do is to use all_different(List).&lt;/p&gt;

&lt;p&gt;The full code is available at &lt;a href=&quot;http://github.com/blackecho/prolog-programs&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Thu, 14 May 2015 21:30:20 +0200</pubDate>
        <link>http://www.gabrieleangeletti.com/blog/prolog/2015/05/14/sudoku-csp-prolog.html</link>
        <guid isPermaLink="true">http://www.gabrieleangeletti.com/blog/prolog/2015/05/14/sudoku-csp-prolog.html</guid>
        
        
        <category>blog</category>
        
        <category>prolog</category>
        
      </item>
    
  </channel>
</rss>
